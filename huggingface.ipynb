{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Potato is a starchy vegetable that is grown underground in rows in a shallow, well-drained soil. It is a tuber, which means it is an underground, fleshy part of the plant that stores nutrients. Potatoes are a popular food source and are used in many different dishes, including salads, soups, stews, and baked dishes. They are also used as a source of starch for making paper and other products. Potatoes are high in carbohydrates and are a good source of fiber, vitamin C, and potassium.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"[INST]What is the meaning of {word}[/INST]\")\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\":250,\n",
    "    }\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke({\n",
    "    \"word\":\"potato\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 21.9kB/s]\n",
      "Downloading config.json: 100%|██████████| 665/665 [00:00<00:00, 3.66MB/s]\n",
      "Downloading vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.46MB/s]\n",
      "Downloading merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 71.4MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 1.36M/1.36M [00:01<00:00, 1.26MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 548M/548M [00:09<00:00, 55.5MB/s] \n",
      "Downloading generation_config.json: 100%|██████████| 124/124 [00:00<00:00, 228kB/s]\n",
      "Using pad_token, but it is not set yet.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' small edible, often called a \"potato plant\" if your kitchen uses potatoes.\\n\\nTo make the potato, use white sauce or a nonstick cooking spray, and place the potatoes in the coldest spot of the oven – under the bedside table where they are stored for at'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"A {word} is a\")\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"openai-community/gpt2\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\":58\n",
    "    }\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke({\n",
    "    \"word\":\"potato\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLModel ERROR: CPU does not support AVX\n",
      "Invalid model file\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for GPT4All\n__root__\n  Unable to instantiate model (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/terrydawunhan/Laptop/STUDY/fullstack-gpt/huggingface.ipynb 셀 3\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/terrydawunhan/Laptop/STUDY/fullstack-gpt/huggingface.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprompts\u001b[39;00m \u001b[39mimport\u001b[39;00m PromptTemplate\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/terrydawunhan/Laptop/STUDY/fullstack-gpt/huggingface.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m prompt \u001b[39m=\u001b[39m PromptTemplate\u001b[39m.\u001b[39mfrom_template(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/terrydawunhan/Laptop/STUDY/fullstack-gpt/huggingface.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mYou are a helpful assistant that defines words. Define this word: \u001b[39m\u001b[39m{word}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/terrydawunhan/Laptop/STUDY/fullstack-gpt/huggingface.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/terrydawunhan/Laptop/STUDY/fullstack-gpt/huggingface.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m llm \u001b[39m=\u001b[39m GPT4All(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/terrydawunhan/Laptop/STUDY/fullstack-gpt/huggingface.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./falcon.bin\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/terrydawunhan/Laptop/STUDY/fullstack-gpt/huggingface.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpu\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/terrydawunhan/Laptop/STUDY/fullstack-gpt/huggingface.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/terrydawunhan/Laptop/STUDY/fullstack-gpt/huggingface.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m chain \u001b[39m=\u001b[39m prompt \u001b[39m|\u001b[39m llm\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/terrydawunhan/Laptop/STUDY/fullstack-gpt/huggingface.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m chain\u001b[39m.\u001b[39minvoke({\u001b[39m\"\u001b[39m\u001b[39mword\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtomato\u001b[39m\u001b[39m\"\u001b[39m})\n",
      "File \u001b[0;32m~/Laptop/STUDY/fullstack-gpt/env/lib/python3.11/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/Laptop/STUDY/fullstack-gpt/env/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for GPT4All\n__root__\n  Unable to instantiate model (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain.llms.gpt4all import GPT4All\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"You are a helpful assistant that defines words. Define this word: {word}.\"\n",
    ")\n",
    "\n",
    "llm = GPT4All(\n",
    "    model=\"./falcon.bin\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"word\": \"tomato\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
